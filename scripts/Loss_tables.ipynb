{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd944e0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T01:06:01.852887Z",
     "start_time": "2024-11-23T01:05:58.121335Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3869d79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T01:06:02.794844Z",
     "start_time": "2024-11-23T01:06:02.664785Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = '/Users/shlokkamat/Documents/Documents - Shlokâ€™s MacBook Pro/GitHub/NUS_Proj/SHAP/data/train.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0fd096",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T01:06:03.451696Z",
     "start_time": "2024-11-23T01:06:03.434901Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['Unnamed: 0', 'id', 'Arrival Delay in Minutes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61e54a85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T01:06:03.956383Z",
     "start_time": "2024-11-23T01:06:03.898299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'satisfaction']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655e3a18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T01:06:04.343242Z",
     "start_time": "2024-11-23T01:06:04.336794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X = data.drop(columns=['satisfaction'])\n",
    "y = data['satisfaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da9699bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T01:06:27.485128Z",
     "start_time": "2024-11-23T01:06:06.658035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 35957, number of negative: 47166\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 679\n",
      "[LightGBM] [Info] Number of data points in the train set: 83123, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432576 -> initscore=-0.271350\n",
      "[LightGBM] [Info] Start training from score -0.271350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]) if hasattr(model, \"predict_proba\") else None\n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"ROC-AUC\": roc_auc\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cafef5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T01:06:31.361679Z",
     "start_time": "2024-11-23T01:06:31.357362Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cecb9b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T01:06:31.811869Z",
     "start_time": "2024-11-23T01:06:31.806231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Precision    Recall  F1-Score   ROC-AUC\n",
      "0             CatBoost  0.962754   0.972323  0.941442  0.956634  0.995069\n",
      "1        Random Forest  0.963380   0.975283  0.939899  0.957264  0.994055\n",
      "2        Decision Tree  0.946682   0.938906  0.938906  0.938906  0.945804\n",
      "3             LightGBM  0.962995   0.974717  0.939568  0.956820  0.994846\n",
      "4  Logistic Regression  0.869400   0.856886  0.841200  0.848971  0.923894\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0945b1a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T01:12:28.164437Z",
     "start_time": "2024-11-23T01:11:12.675456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 35957, number of negative: 47166\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 679\n",
      "[LightGBM] [Info] Number of data points in the train set: 83123, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432576 -> initscore=-0.271350\n",
      "[LightGBM] [Info] Start training from score -0.271350\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 83123.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 679\n",
      "[LightGBM] [Info] Number of data points in the train set: 83123, number of used features: 21\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.432576 -> initscore = -0.271350\n",
      "[LightGBM] [Info] Start training from score -0.271350\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] Unknown objective type name: focal_loss\n",
      "[LightGBM] [Fatal] Number of classes should be specified and greater than 1 for multiclass training\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Model  Accuracy  F1 Score  \\\n",
      "0                  CatBoost (Logloss)  0.962754  0.956634   \n",
      "1             CatBoost (CrossEntropy)  0.963428  0.957399   \n",
      "2               CatBoost (MultiClass)  0.961985  0.955757   \n",
      "3           LightGBM (Binary Logloss)  0.962995  0.956820   \n",
      "4            LightGBM (Cross-Entropy)  0.962995  0.956820   \n",
      "5               LightGBM (Focal Loss)       NaN       NaN   \n",
      "6               LightGBM (MultiClass)       NaN       NaN   \n",
      "7             Random Forest (Default)  0.963380  0.957264   \n",
      "8             Random Forest (Entropy)  0.962803  0.956575   \n",
      "9             Random Forest (Logloss)  0.962803  0.956575   \n",
      "10            Decision Tree (Default)  0.946682  0.938906   \n",
      "11            Decision Tree (Entropy)  0.948126  0.940541   \n",
      "12            Decision Tree (Logloss)  0.948126  0.940541   \n",
      "13      Logistic Regression (Default)  0.869400  0.848971   \n",
      "14     Logistic Regression (Balanced)  0.864877  0.847970   \n",
      "15   Logistic Regression (L1 Penalty)  0.877965  0.857288   \n",
      "16  Logistic Regression (Elastic Net)  0.822626  0.798711   \n",
      "\n",
      "                                                Error  \n",
      "0                                                 NaN  \n",
      "1                                                 NaN  \n",
      "2                                                 NaN  \n",
      "3                                                 NaN  \n",
      "4                                                 NaN  \n",
      "5             Unknown objective type name: focal_loss  \n",
      "6   Number of classes should be specified and grea...  \n",
      "7                                                 NaN  \n",
      "8                                                 NaN  \n",
      "9                                                 NaN  \n",
      "10                                                NaN  \n",
      "11                                                NaN  \n",
      "12                                                NaN  \n",
      "13                                                NaN  \n",
      "14                                                NaN  \n",
      "15                                                NaN  \n",
      "16                                                NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models with more loss/objective functions\n",
    "models = {\n",
    "    \"CatBoost (Logloss)\": CatBoostClassifier(loss_function='Logloss', verbose=0, random_state=42),\n",
    "    \"CatBoost (CrossEntropy)\": CatBoostClassifier(loss_function='CrossEntropy', verbose=0, random_state=42),\n",
    "    \"CatBoost (MultiClass)\": CatBoostClassifier(loss_function='MultiClass', verbose=0, random_state=42),\n",
    "    \"LightGBM (Binary Logloss)\": lgb.LGBMClassifier(objective='binary', random_state=42),\n",
    "    \"LightGBM (Cross-Entropy)\": lgb.LGBMClassifier(objective='cross_entropy', random_state=42),\n",
    "    \"LightGBM (Focal Loss)\": lgb.LGBMClassifier(objective='focal_loss', random_state=42),\n",
    "    \"LightGBM (MultiClass)\": lgb.LGBMClassifier(objective='multiclass', random_state=42),\n",
    "    \"Random Forest (Default)\": RandomForestClassifier(random_state=42),\n",
    "    \"Random Forest (Entropy)\": RandomForestClassifier(random_state=42, criterion = 'entropy'),\n",
    "    \"Random Forest (Logloss)\": RandomForestClassifier(random_state=42, criterion = 'log_loss'),\n",
    "    \"Decision Tree (Default)\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Decision Tree (Entropy)\": DecisionTreeClassifier(random_state=42, criterion = 'entropy'),\n",
    "    \"Decision Tree (Logloss)\": DecisionTreeClassifier(random_state=42, criterion = 'log_loss'),\n",
    "    \"Logistic Regression (Default)\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Logistic Regression (Balanced)\": LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "    \"Logistic Regression (L1 Penalty)\": LogisticRegression(max_iter=1000, penalty='l1', solver='liblinear', random_state=42),\n",
    "    \"Logistic Regression (Elastic Net)\": LogisticRegression(max_iter=1000, penalty='elasticnet', solver='saga', l1_ratio=0.5, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    try:\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Evaluate performance\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        # Append results\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"F1 Score\": f1\n",
    "        })\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": None,\n",
    "            \"Error\": str(e),\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Model Performance with Different Objectives\", dataframe=results_df)\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba6075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
